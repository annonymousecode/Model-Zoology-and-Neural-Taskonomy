{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {padding: 0; border: 0;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>.p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {padding: 0; border: 0;}</style>\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os, sys, pickle\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Statistics By Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dir = 'response_arrays/'\n",
    "cell_metadata = pd.read_csv(os.path.join(response_dir, 'cell_metadata.csv')\n",
    "cell_responses = pickle.load(open(os.path.join(response_dir, 'response_average_bycell.pkl', 'rb')))\n",
    "cell_responses_bytrial = pickle.load(open(os.path.join(response_dir, 'responses_bytrial_bycell.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.reliability import split_half\n",
    "\n",
    "output_file = 'reliabilities_bycell.csv'\n",
    "if os.path.exists(output_file):\n",
    "    neural_reliabilities = pd.read_csv(output_file)\n",
    "    \n",
    "if not os.path.exists(output_file):\n",
    "    neural_reliabilities_dictlist = []\n",
    "    for cell_id in tqdm(cell_metadata['cell_specimen_id']):\n",
    "        reference_array = cell_responses[cell_id]\n",
    "        response_array = cell_responses_bytrial[cell_id]\n",
    "        response_by_image_array = np.zeros(shape=(response_array.shape[0] // 50, 50))\n",
    "        for image in range(response_array.shape[0] // 50):\n",
    "            response_by_image_array[image] = response_array[50*image:50*(image+1)]\n",
    "        image_by_response_array = np.swapaxes(response_by_image_array, 0, 1)    \n",
    "        response_reliability = split_half(image_by_response_array)[0]\n",
    "        neural_reliabilities_dictlist.append({'cell_specimen_id': cell_id, 'splithalf_r': response_reliability})\n",
    "        if not np.allclose(reference_array, response_by_image_array.mean(axis=1)):\n",
    "            print(cell_id)\n",
    "\n",
    "    neural_reliabilities = pd.DataFrame(neural_reliabilities_dictlist)\n",
    "    neural_reliabilities.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_specimen_id</th>\n",
       "      <th>splithalf_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>662220119</td>\n",
       "      <td>0.820679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662220063</td>\n",
       "      <td>0.879785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>662220048</td>\n",
       "      <td>0.966658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>662220001</td>\n",
       "      <td>0.918633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>662220053</td>\n",
       "      <td>0.800464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39242</th>\n",
       "      <td>662225725</td>\n",
       "      <td>0.775695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39243</th>\n",
       "      <td>662223605</td>\n",
       "      <td>0.812394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39247</th>\n",
       "      <td>662222176</td>\n",
       "      <td>0.948436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39262</th>\n",
       "      <td>517504625</td>\n",
       "      <td>0.871873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39303</th>\n",
       "      <td>517503813</td>\n",
       "      <td>0.986910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8387 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cell_specimen_id  splithalf_r\n",
       "1             662220119     0.820679\n",
       "2             662220063     0.879785\n",
       "5             662220048     0.966658\n",
       "11            662220001     0.918633\n",
       "12            662220053     0.800464\n",
       "...                 ...          ...\n",
       "39242         662225725     0.775695\n",
       "39243         662223605     0.812394\n",
       "39247         662222176     0.948436\n",
       "39262         517504625     0.871873\n",
       "39303         517503813     0.986910\n",
       "\n",
       "[8387 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_reliabilities.query('splithalf_r > 0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_data_plus = cell_metadata.merge(neural_reliabilities, on=['cell_specimen_id'])\n",
    "\n",
    "output_file = 'cell_data_plus.csv'\n",
    "if not os.path.exists(output_file):\n",
    "    cell_data_plus.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Statistics By Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before anything else, we ensure our arrays are properly shaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitatory = ['Emx1', 'Slc17a7', 'Cux2', 'Rorb', 'Scnn1a', 'Nr5a1', 'Rbp4', 'Fezf2', 'Tlx3', 'Ntsr1']\n",
    "inhibitory = ['Sst', 'Vip', 'Pvalb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('responses_bytrial_bysite.pkl', 'rb') as f:\n",
    "    brain_responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_areas = list(brain_responses.keys())\n",
    "layers = ['layer23', 'layer4', 'layer5', 'layer6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_response_summary = {}\n",
    "for area in visual_areas:\n",
    "    brain_response_summary[area] = {}\n",
    "    for layer in layers:\n",
    "        brain_response_summary[area][layer] = []\n",
    "        for cre_line in list(brain_responses[area].keys()):\n",
    "            if cre_line not in inhibitory:\n",
    "                if layer in brain_responses[area][cre_line].keys():\n",
    "                    #brain_response_summary[area][layer].append(brain_responses[area][cre_line][layer].transpose())\n",
    "                    brain_response_summary[area][layer].append(brain_responses[area][cre_line][layer])\n",
    "        if len(brain_response_summary[area][layer]) == 0:\n",
    "            brain_response_summary[area].pop(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_responses_bytrial = {}\n",
    "for area in visual_areas:\n",
    "    brain_responses_bytrial[area] = {}\n",
    "    for layer in layers:\n",
    "        if layer in brain_response_summary[area].keys():\n",
    "            brain_responses_bytrial[area][layer] = np.hstack(brain_response_summary[area][layer])\n",
    "            #print(area, layer, os.linesep, brain_responses_bytrial[area][layer].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(response_dir, 'response_average_bysite.pkl', 'rb')) as file:\n",
    "    response_dict = pickle.load(file)\n",
    "\n",
    "brain_responses_bytrial_aggregate = []\n",
    "for image_index in range(5950 // 50):\n",
    "    brain_responses_bytrial_aggregate.append(np.mean(brain_responses_bytrial['VISl']['layer23'][50*image_index:50*(image_index+1), :], axis = 0))\n",
    "brain_responses_bytrial_aggregate = np.vstack(brain_responses_bytrial_aggregate)\n",
    "\n",
    "brain_responses_byimage = {}\n",
    "for area in response_dict.keys():\n",
    "    resp_exc = {'layer23':[], 'layer4':[], 'layer5':[], 'layer6':[]}\n",
    "    resp_inh = {'layer23':[], 'layer4':[], 'layer5':[], 'layer6':[]}\n",
    "    brain_responses_byimage[area] = {}\n",
    "    for cre in response_dict[area].keys():\n",
    "        for layer in response_dict[area][cre].keys():\n",
    "            #print(area, cre, layer, os.linesep, response_dict[area][cre][layer].shape)\n",
    "\n",
    "            response = response_dict[area][cre][layer]\n",
    "\n",
    "            if cre in excitatory:\n",
    "                resp_exc[layer] += [response]\n",
    "            if cre in inhibitory:\n",
    "                resp_inh[layer] += [response]\n",
    "                \n",
    "    for layer in resp_exc:\n",
    "        \n",
    "        if len(resp_exc[layer])>0:\n",
    "            resp_e = np.hstack(resp_exc[layer])\n",
    "            brain_responses_byimage[area][layer] = resp_e\n",
    "            #print(area, layer, os.linesep, resp_e.shape)\n",
    "            \n",
    "print('Trial arrays properly computed?', np.array_equal(brain_responses_bytrial_aggregate, brain_responses_byimage['VISl']['layer23']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_responses_bytrial['VISl']['layer23'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_responses_byimage['VISl']['layer23'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.reliability import split_half\n",
    "\n",
    "output_file = 'reliabilities_bysite.pkl'\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'rb') as file:\n",
    "        neural_reliabilities = pickle.load(file)\n",
    "    \n",
    "if not os.path.exists(output_file):\n",
    "    neural_reliabilities = {}\n",
    "    for area in tqdm(visual_areas):\n",
    "        neural_reliabilities[area] = {}\n",
    "        layers = brain_responses_bytrial[area].keys()\n",
    "        for layer in tqdm(layers, leave = False):\n",
    "            response_array = brain_responses_bytrial[area][layer]\n",
    "            neural_array = np.zeros(shape=(response_array.shape[1], response_array.shape[0] // 50, 50))\n",
    "            for neuron in range(response_array.shape[1]):\n",
    "                for image in range(response_array.shape[0] // 50):\n",
    "                    neural_array[neuron][image] = response_array[50*image:50*(image+1), neuron]\n",
    "\n",
    "            neural_array = np.swapaxes(neural_array, 1, 2)\n",
    "            neural_reliability = np.zeros((neural_array.shape[0], 2))\n",
    "            for neuron in tqdm(range(neural_array.shape[0]), desc = ' '.join([area, layer]), leave = False):\n",
    "                neural_reliability[neuron] = split_half(neural_array[neuron,:,:])\n",
    "\n",
    "            neural_reliabilities[area][layer] = neural_reliability\n",
    "            \n",
    "    neural_reliabilities_dictlist = []\n",
    "    for area in list(neural_reliabilities.keys()):\n",
    "        for layer in list(neural_reliabilities[area].keys()):\n",
    "            for neural_index, neuron in enumerate(neural_reliabilities[area][layer]):\n",
    "                neural_reliabilities_dictlist.append({'area' : area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                                      'neuron': neural_index, 'splithalf_r': neuron[0], 'splithalf_sem': neuron[0]})\n",
    "    \n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(neural_reliabilities, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_reliabilities_dictlist = []\n",
    "for area in list(neural_reliabilities.keys()):\n",
    "    for layer in list(neural_reliabilities[area].keys()):\n",
    "        for neural_index, neuron in enumerate(neural_reliabilities[area][layer]):\n",
    "            neural_reliabilities_dictlist.append({'area' : area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                                  'neuron': neural_index, 'splithalf_r': neuron[0], 'splithalf_sem': neuron[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_bysite_dictlist = []\n",
    "for area in visual_areas:\n",
    "    for layer in neural_reliabilities[area].keys():\n",
    "        reliability = np.mean(neural_reliabilities[area][layer], axis = 0)\n",
    "        reliability_bysite_dictlist.append({'area': area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                            'number_of_neurons': neural_reliabilities[area][layer].shape[0],\n",
    "                                            'reliability': reliability[0], 'sem': reliability[1]})\n",
    "\n",
    "reliability_bysite = pd.DataFrame(reliability_bysite_dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "p = sns.regplot(x='number_of_neurons', y='reliability', data=reliability_bysite);\n",
    "\n",
    "for point in range(0, reliability_bysite.shape[0]):\n",
    "     p.text(reliability_bysite['number_of_neurons'][point]+0.01, reliability_bysite['reliability'][point], \n",
    "     reliability_bysite['neural_site'][point], horizontalalignment='left', \n",
    "     size='medium', color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_neurons = 0\n",
    "for area in visual_areas:\n",
    "    for layer in neural_reliabilities[area].keys():\n",
    "        total_neurons += neural_reliabilities[area][layer].shape[0]\n",
    "        print(area, layer, ':', neural_reliabilities[area][layer].shape[0])\n",
    "print('Total neurons: ', total_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_count_list = []\n",
    "for cutoff in tqdm(np.linspace(0,0.99,100)):\n",
    "    reliable_neurons = {}\n",
    "    for area in visual_areas:\n",
    "        reliable_neurons[area] = {}\n",
    "        for layer in neural_reliabilities[area].keys():\n",
    "            reliable_neurons[area][layer] = []\n",
    "            reliable_neurons_in_layer = 0\n",
    "            for neural_index, neuron in enumerate(neural_reliabilities[area][layer]):\n",
    "                if neuron[0] >= cutoff:\n",
    "                    reliable_neurons[area][layer].append(neural_index)\n",
    "            reliable_neurons[area][layer] = np.array(reliable_neurons[area][layer])\n",
    "            reliability_count_list.append({'area': area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                           'cutoff': cutoff, 'count': reliable_neurons[area][layer].shape[0]})\n",
    "            \n",
    "reliability_counts = pd.DataFrame(reliability_count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.relplot(x='cutoff', y='count', hue='neural_site', data = reliability_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='cutoff', y='count', data = reliability_counts.groupby('cutoff')['count'].sum().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def spearman_brown(pearson_r):\n",
    "    return 2.0 * pearson_r / (1.0 + pearson_r)\n",
    "\n",
    "output_file = 'reliability_selection_bysite.csv'\n",
    "if os.path.exists(output_file):\n",
    "    reliabilities = pd.read_csv(output_file)\n",
    "    \n",
    "if not (os.path.exists(output_file)):\n",
    "    reliabilities_list = []\n",
    "    for cutoff in tqdm(np.linspace(0.0,0.99,100)):\n",
    "        for area in tqdm(visual_areas, leave = False):\n",
    "            for layer in tqdm(list(brain_responses_bytrial[area].keys()), leave = False):\n",
    "                reliable_neurons = [i for (i, el) in enumerate(neural_reliabilities[area][layer]) if el[0] >= cutoff]\n",
    "                response_array = brain_responses_bytrial[area][layer][:, reliable_neurons]\n",
    "                neural_array = np.zeros(shape=(response_array.shape[1], response_array.shape[0] // 50, 50))\n",
    "                for neuron in range(response_array.shape[1]):\n",
    "                    for image in range(response_array.shape[0] // 50):\n",
    "                        neural_array[neuron][image] = response_array[50*image:50*(image+1), neuron]\n",
    "                        \n",
    "                iter_dict1 = {'cutoff': cutoff, 'area': area, 'layer': layer, \n",
    "                             'neural_site': area+'-'+layer, 'neuron_count': len(reliable_neurons)}\n",
    "                trial_indices = np.array(range(50))\n",
    "                neural_array = np.swapaxes(neural_array,0,2)\n",
    "                if neural_array.shape[2] > 3:\n",
    "                    for i in range(10):\n",
    "                        np.random.shuffle(trial_indices)\n",
    "                        similarity1 = np.corrcoef(np.mean(neural_array[trial_indices[:25], :, : ], axis=0))\n",
    "                        similarity2 = np.corrcoef(np.mean(neural_array[trial_indices[25:], :, : ], axis=0))\n",
    "                        sim1_triu = similarity1[np.triu_indices(similarity1.shape[0], k=1)].flatten()\n",
    "                        sim2_triu = similarity2[np.triu_indices(similarity2.shape[0], k=1)].flatten()\n",
    "                        rdm_pearson = pearsonr(np.nan_to_num(sim1_triu), np.nan_to_num(sim2_triu))[0]\n",
    "                        iter_dict2 = {'iteration': i, 'instance': 'rdm', \n",
    "                                      'splithalf_r': rdm_pearson, 'spearman_brown': spearman_brown(rdm_pearson)}\n",
    "                        reliabilities_list.append({**iter_dict1, **iter_dict2})\n",
    "\n",
    "                        image_splitcorrs = np.zeros(119)\n",
    "                        for image in range(neural_array.shape[1]):\n",
    "                            image_splithalf1 = np.mean(neural_array[trial_indices[:25], image, :], axis = 0)\n",
    "                            image_splithalf2 = np.mean(neural_array[trial_indices[25:], image, :], axis = 0)\n",
    "                            image_pearson = pearsonr(np.nan_to_num(image_splithalf1), np.nan_to_num(image_splithalf2))[0]\n",
    "                            iter_dict2 = {'iteration': i, 'instance': 'image' + str(image + 1),\n",
    "                                          'splithalf_r': image_pearson, 'spearman_brown': spearman_brown(image_pearson)}\n",
    "                            reliabilities_list.append({**iter_dict1, **iter_dict2})\n",
    "        \n",
    "    reliabilities = pd.DataFrame(reliabilities_list)\n",
    "    reliabilities.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_reliabilities = reliabilities[reliabilities['instance'].str.contains('rdm')]\n",
    "image_reliabilities = reliabilities[reliabilities['instance'].str.contains('image')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reliability Selection Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='cutoff', y='splithalf_r', hue='neural_site', kind='line', \n",
    "            data=rdm_reliabilities.groupby(['cutoff','neural_site'])['splithalf_r'].mean().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='cutoff', y='splithalf_r', row='layer', col='area', hue='neural_site', kind='line', \n",
    "            data=rdm_reliabilities.groupby(['cutoff','area', 'layer', 'neural_site'])['splithalf_r'].mean().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='cutoff', y='splithalf_r', hue='neural_site', kind='line', \n",
    "            data=image_reliabilities.groupby(['cutoff','neural_site', 'instance'])['splithalf_r'].mean().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='cutoff', y='splithalf_r', row='layer', col='area', hue='neural_site', kind='line', \n",
    "            data=image_reliabilities.groupby(['cutoff','area', 'layer', 'neural_site', 'instance'])['splithalf_r'].mean().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_neuron_count = 0\n",
    "reliable_neurons = {}\n",
    "for area in visual_areas:\n",
    "    reliable_neurons[area] = {}\n",
    "    for layer in neural_reliabilities[area].keys():\n",
    "        reliable_neurons[area][layer] = []\n",
    "        reliable_neurons_in_layer = 0\n",
    "        for neural_index, neuron in enumerate(neural_reliabilities[area][layer]):\n",
    "            if neuron[0] >= 0.75:\n",
    "                reliable_neuron_count += 1\n",
    "                reliable_neurons_in_layer += 1\n",
    "                reliable_neurons[area][layer].append(neural_index)\n",
    "        reliable_neurons[area][layer] = np.array(reliable_neurons[area][layer])\n",
    "        print(area, layer, ':', reliable_neurons_in_layer)\n",
    "print('total reliable neurons: ', reliable_neuron_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_reliabilities_df.query('splithalf_r >= 0.75').groupby(['area','layer'])['neuron'].count()\n",
    "print('total reliable neurons: ', neural_reliabilities_df.query('splithalf_r > 0.75')['neuron'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_brain_rdms = {}\n",
    "reliable_responses_byimage = {}\n",
    "reliable_responses_bytrial = {}\n",
    "for area in visual_areas:\n",
    "    reliable_brain_rdms[area] = {}\n",
    "    reliable_responses_byimage[area] = {}\n",
    "    reliable_responses_bytrial[area] = {}\n",
    "    for layer in neural_reliabilities[area].keys():\n",
    "        reliable_responses_byimage[area][layer]=brain_responses_byimage[area][layer][:,reliable_neurons[area][layer]]\n",
    "        reliable_responses_bytrial[area][layer]=brain_responses_bytrial[area][layer][:,reliable_neurons[area][layer]]\n",
    "        reliable_brain_rdms[area][layer] = np.corrcoef(reliable_responses_byimage[area][layer])\n",
    "        \n",
    "output_file = 'rdms_reliable.pkl'\n",
    "if not os.path.exists(output_file): \n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(reliable_brain_rdms, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "target_responses = reliable_responses_bytrial\n",
    "\n",
    "trial_indices = np.array(range(50))\n",
    "neural_splithalf_rdms = {}\n",
    "for area in tqdm(visual_areas):\n",
    "    neural_splithalf_rdms[area] = {}\n",
    "    layers = list(target_responses[area].keys())\n",
    "    for layer in tqdm(layers, leave = False):\n",
    "        neural_splithalf_rdms[area][layer] = {}\n",
    "        response_array = target_responses[area][layer]\n",
    "        neural_array = np.zeros(shape=(response_array.shape[1], response_array.shape[0] // 50, 50))\n",
    "        for neuron in range(response_array.shape[1]):\n",
    "                for image in range(response_array.shape[0] // 50):\n",
    "                    neural_array[neuron][image] = response_array[50*image:50*(image+1), neuron]\n",
    "                    \n",
    "        neural_array = np.swapaxes(neural_array, 0, 2)\n",
    "        for iteration in tqdm(range(10), desc = 'Similarity', leave = False):\n",
    "            np.random.shuffle(trial_indices)\n",
    "            similarity1 = np.corrcoef(np.mean(neural_array[trial_indices[:25], :, : ], axis=0))\n",
    "            similarity2 = np.corrcoef(np.mean(neural_array[trial_indices[25:], :, : ], axis=0))\n",
    "            sim1_triu = similarity1[np.triu_indices(similarity1.shape[0], k=1)].flatten()\n",
    "            sim2_triu = similarity2[np.triu_indices(similarity2.shape[0], k=1)].flatten()\n",
    "            sim_corr = pearsonr(np.nan_to_num(sim1_triu), np.nan_to_num(sim2_triu))[0]\n",
    "            neural_splithalf_rdms[area][layer][iteration] = 2.0 * sim_corr / (1.0 + sim_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictlist = []\n",
    "for area in neural_splithalf_rdms.keys():\n",
    "    for layer in neural_splithalf_rdms[area].keys():\n",
    "        for iteration in neural_splithalf_rdms[area][layer].keys():\n",
    "            dictlist.append({'area': area, 'layer': layer, 'similarity': neural_splithalf_rdms[area][layer][iteration], \n",
    "                             'id': 'neural_internal_' + str(iteration)})\n",
    "            \n",
    "neural_internal_rdm_summary = pd.DataFrame(dictlist)\n",
    "\n",
    "output_file = 'rdms_splithalf.csv'\n",
    "if not os.path.exists(output_file):\n",
    "     neural_internal_rdm_summary.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='area', y='similarity', hue='layer', kind='bar', palette=sns.light_palette('navy'), \n",
    "            data=neural_internal_rdm_summary);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats_dictlist = []\n",
    "for area in tqdm(visual_areas):\n",
    "    layers = brain_responses_byimage[area].keys()\n",
    "    for layer in tqdm(layers, leave = False):\n",
    "        for neuron in range(brain_responses_byimage[area][layer].shape[1]):\n",
    "            neural_mean = np.mean(brain_responses_byimage[area][layer][:,neuron])\n",
    "            neural_sigma = np.var(brain_responses_byimage[area][layer][:,neuron])\n",
    "            neural_stats_dictlist.append({'area': area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                          'neuron': neuron, 'mu': neural_mean, 'sigma': neural_sigma})\n",
    "                \n",
    "neural_stats = pd.DataFrame(neural_stats_dictlist)\n",
    "output_file = 'summary_stats.csv'\n",
    "if not os.path.exists(output_file):\n",
    "    neural_stats.to_csv(output_file, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats.groupby(['area', 'layer'])['mu'].describe()[['count','mean','std','min','max']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats.groupby(['area', 'layer'])['sigma'].describe()[['count','mean','std','min','max']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(neural_stats['mu'], bins = 100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(neural_stats['sigma'], bins = 100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "area = copy(visual_areas)\n",
    "layers = ['layer23', 'layer4', 'layer5', 'layer6']\n",
    "fig, axs = plt.subplots(4, 6, figsize=(16,8))\n",
    "for i, area in enumerate(visual_areas):\n",
    "    for j, layer in enumerate(layers):\n",
    "        data = neural_stats[(neural_stats.area == area) & (neural_stats.layer == layer)]\n",
    "        axs[j, i].hist(data['sigma'], bins = 100)\n",
    "        axs[j, i].set_yscale('log')\n",
    "        axs[j, i].set_title(area +'-'+layer)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_minmax_norm(df):\n",
    "    return (df-df.min())/(df.max()-df.min())\n",
    "\n",
    "neural_stats[['mu_norm', 'sigma_norm']] = pd_minmax_norm(neural_stats[['mu', 'sigma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats['sigma'].mean(), neural_stats['sigma'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats['sigma_norm'].mean(), neural_stats['sigma_norm'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats[neural_stats['sigma_norm'] > .005]['neuron'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats[['mu_norm', 'sigma_norm']] = pd_minmax_norm(neural_stats[['mu', 'sigma']])\n",
    "variance_count_list = []\n",
    "for cutoff in tqdm(np.linspace(neural_stats['sigma_norm'].min(), .0001, 100)):\n",
    "    for area in tqdm(visual_areas, leave = False):\n",
    "        layers = brain_responses_byimage[area].keys()\n",
    "        for layer in tqdm(layers, leave = False):\n",
    "            neural_subset = neural_stats[(neural_stats['area'] == area) & (neural_stats['layer'] == layer)]\n",
    "            neuron_count = neural_subset[neural_subset['sigma_norm'] > cutoff]['neuron'].count()\n",
    "            variance_count_list.append({'area': area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                           'cutoff': cutoff, 'count': neuron_count})\n",
    "variance_counts = pd.DataFrame(variance_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='cutoff', y='count', hue='neural_site', data = variance_counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='cutoff', y='count', data = variance_counts.groupby('cutoff')['count'].sum().reset_index());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_stats[['mu_norm', 'sigma_norm']] = pd_minmax_norm(neural_stats[['mu', 'sigma']])\n",
    "variance_count_list = []\n",
    "for cutoff in tqdm(np.linspace(0,1,100)):\n",
    "    for area in tqdm(visual_areas, leave = False):\n",
    "        layers = brain_responses_byimage[area].keys()\n",
    "        for layer in tqdm(layers, leave = False):\n",
    "            neural_subset = neural_stats[(neural_stats['area'] == area) & (neural_stats['layer'] == layer)]\n",
    "            #lower_bound = neural_subset['sigma_norm'].mean() - neural_subset['sigma_norm'].std()*cutoff\n",
    "            #upper_bound = neural_subset['sigma_norm'].mean() + neural_subset['sigma_norm'].std()*cutoff\n",
    "            #lower_bound_conditional = neural_subset['sigma_norm'] > lower_bound\n",
    "            #upper_bound_conditional = neural_subset['sigma_norm'] < upper_bound\n",
    "            #neuron_count = neural_subset[(lower_bound_conditional) & (upper_bound_conditional)]['neuron'].count()\n",
    "            lower_bound = 0 + neural_subset['sigma_norm'].std()*cutoff\n",
    "            neuron_count = neural_subset[neural_subset['sigma_norm'] > lower_bound]['neuron'].count()\n",
    "            variance_count_list.append({'area': area, 'layer': layer, 'neural_site': area+'-'+layer,\n",
    "                                           'cutoff': cutoff, 'count': neuron_count})\n",
    "variance_counts = pd.DataFrame(variance_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_data = variance_counts.groupby('cutoff')['count'].sum().reset_index()\n",
    "xdata = curve_data['cutoff']\n",
    "ydata = curve_data['count']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
